# Conditional-N-gram-Regularization

This repository contains the code used for one Research paper:

[Using Large Corpus N-gram Statistics to Improve Recurrent Neural Language Models](https://www.aclweb.org/anthology/papers/N/N19/N19-1330/)

This code was originally forked from the [PyTorch word level language modeling example](https://github.com/pytorch/examples/tree/master/word_language_model) and the variational dropout implementation is from [LSTM and QRNN Language Model Toolkit](https://github.com/salesforce/awd-lstm-lm).

The code is currently under cleaning and will be done soon.
